\section{Preliminaries}
\label{section:preliminaries}

The section provides some backgraound, definitions, and theoremes required for
developed algortithms.

\subsection{Approximate pattern matching}
The approximate pattern matching problem ($AMatch$) defined as follows.
Given text $t$, pattern $p$, simularity function $g$, and some threshold $h$ the \emph{approximate pattern matching} problem asks for all substrings from text $t$ that have similarity score  with given pattern $p$ at least $h$ according to a function $g$.  

There exist different extensions and particular cases of the problem.
The most familiar case, \emph{complete approximate pattern matching} (\emph{CompleteAMatch}) that asks for all substrings of text $t$ that are exact clones of pattern $p$.
CompleteAMatch can be solved by a number well-knonwn algorithms such as Aho--Korasic, Bouer--Murr, Knuth--Morris--Pratt, and so on.
The optimal CompleteAMatch solution running time is $O(|p|+|t|)$~\cite{.}.
Other special cases of AMatch are \emph{approximate pattern matching with k mismatches}~\cite{.}, search of \emph{pattern with wildcard symbols}~\cite{.}, multidimensional \emph{AMatch}~\cite{.}, AMatch with a length constraint on the resulting substrings~\cite{.}, and many more~\cite{.}.

One of the common approaches to solving AMAtch problem is the utilization of string similarity problem solutions.
Latter represents a set of fundamental problems such as \emph{edit distance}, \emph{longest common subsequence}, and \emph{sequence alignment}.
During algorithms design, we primarily focused on the usage of the latter two.

In some approximate pattern matching applications, it may be crucial to impose a constraint on length of the matched substrings.
In bioinformatics, for example, substrings with small size may be less important in a biological sense then substrings with less similarity score but much larger length~\cite{.}.
Same is also true for the length upper bound.
Besides one may use length constraints in clone detection in software code and documentation tools in order to decrease false-positive rate~\cite{.}.
Recently there has been developed an algorithm for solving this particular \emph{AMatch} extension~\cite{.}.
Despite the algorithm has poor time complexity, the proposed solution possesses a completeness property, i.e it founds \emph{all} non-intersected clones of a given pattern with specified similarity threshold and length constraint on matching substrings.
Precisely due to the completeness property the algorithm is of intereset in this paper.
The algorithm is described in section~\ref{.} while the developed improved version is presented in section~\ref{.}.

\subsection{Global lcs and sa}
% Semi-local lcs and sa are extensions of \emph{lcs} and \emph{sa} problems, respectively.

\begin{definition}
Given two strings $a$ and $b$  the longest common subsequence (\emph{LCS}) problem ask for the maximal length of the longest common subsequence (\emph{lcs} score) of $a$ and $b$ ($lcs(a,b)$).
\end{definition} 
%\todo{In other words, \emph{LCS} problem asks about maximal %$lcs$ score of two given string $a$ and $b$ ($lcs(a,b)$)}.

\begin{definition}
Given two strings $a$ and $b$ and scoring scheme $w=(w_{+},w_{0},w_{-})$ the sequence alignment (\emph{SA}) problem ask for the maximal alignment score between $a$ and $b$ ($sa(a,b,w)$).
\end{definition}

Scoring scheme determines how to calculate the alignment score of two aligned sequences.
If a pair of character in aligned sequences are matched (equal) then this pair contributes to the final alignment score $w_{+}$, if their mismatch it contributes $w_{0}$.
If symbol $\alpha$ of one of the sequences is not aligned with any other symbol from another sequence it means that $\alpha$ is aligned with a $gap$.
In this case, this pair contributes $w_{0}$.
The final scoring scheme calculates as follows:
\begin{equation}\label{formula:sa}
\begin{array}{ll}
    sa(a,b,w) &= w_{+}k^{+} + w_{0}k^{0} + w_{-} (|a| + |b| - 2k^{+} - 2k^{0}) \\
    &= k^{+} (w_{+} - 2w_{-} ) + k^{0}  (w_{0} - 2w_{-}) + w_{-}(|a| + |b|)
\end{array}
\end{equation}
where $k^{+}$ stands for the number of matched symbols, $k^{-}$ --- the number of mismatched symbols.
Note that \emph{LCS} is a special case of \emph{SA} with scoring scheme be $(1,0,0)$.
Both described problems can be solved with usual dynamic programming algorithm having running time complexity $O(|a|\times|b|)$.

\subsection{Semi-local lcs}

$LCS$ and $SA$ allow one to find how much whole given strings are similar, i.e. how similar two string in a global sense.
Sometimes this is not enough.
There exist \emph{fully local} and \emph{semi-local} generalizations of these problems.
In the paper, we focus on \emph{semi-local} ones due to natural applicability to approximate pattern matching.
The key feature of semi-local problems is that they can be solved within the same time as global ones while collects more information about strings similarity.
More precisely, given two strings $a$ and $b$ the semi-local lcs asks about $lcs$ scores for following:
\begin{itemize}
\item \emph{string-substring}: whole $a$ against every substring of $b$,
\item \emph{substring-string}: whole $b$ against every substring of $a$,
\item \emph{prefix-suffix}: every prefix of $a$ against every suffix of $b$,
\item \emph{suffix-prefix}: every prefix of $b$ against every suffix of $a$.
\end{itemize} 
Formally, semi-local lcs can be defined via \emph{semi-local lcs matrix} as follows.
\begin{definition}
The \emph{semi-local lcs matrix}  $H_{a,b}$ for strings $a$, $b$ defined as follows:
\begin{equation}
  H_{a,b}[i,j] = \textbf{ if } j\leq i \textbf{ then } j-i \textbf{ else } lcs(a,b^{pad}[i,j]) 
\end{equation} 
where $i \in [-|a|:|b|], j \in [0:|a|+|b|] $, $b^{pad}= ?^{|a|}\ b\ ?^{|a|}$, $?$--- wildcard symbol that matches any other symbol.
\end{definition}
The semi-local lcs matrix $H_{a,b}$ comprises from four quadrants associated with described subproblems:
\begin{equation}
  H_{a,b} = \begin{bmatrix}
    H_{a,b}^{suf-pre} & H_{a,b}^{sub-str} \\
    H_{a,b}^{str-sub} & H_{a,b}^{pre-suf} 
  \end{bmatrix}    
\end{equation}

Semi-local lcs can be solved within the optimal running time complexity, $O(|a| \times |b|)$, by a number of algorithms based, for example, on fast distance multiplication of unit-Monge matrices~\cite{.} or seaweed combing~\cite{.}.

\subsection{Matrix properties}

\begin{definition}
Matrix $H$ called (anti) Monge matrix if

\begin{displaymath}
  \forall i,i',j,j':\ i\leq i^{'}, j \leq j^{'}\ .\ H[i,j]+H[i^{'},j^{'}] (\geq)\leq H[i,j^{'}]+H[i^{'},j]
\end{displaymath}
\end{definition}

\begin{definition}
Let $H[0:m,0:n]$  be a matrix.
$H^{\square}[0:m-1,0:n-1]$ constructed as a result of taken cross difference between secondary and first diagonal for all adjacent 2 by 2 squares called \emph{cross-difference} matrix of  $H$
\end{definition}


\begin{definition}
Matrix $H$ called unit anti Monge matrix if $H$ is (anti) Monge matrix and its \emph{cross-difference matrix} $(-)H^{\square}$ is permutation matrix.
\end{definition}
The example of unit anti Monge matrix is following:
\begin{equation}
\begin{bmatrix}
0 & 2 & 3 \\
0 & 1 & 2 \\
0 & 1 & 1
\end{bmatrix} ^ { \square} =
\begin{bmatrix}
(2 + 0) - (1 + 0)  & (3 + 1) - (2 + 2)  \\
(1 + 0) - (1 + 0) &  (2 + 1) - (1 + 1) 
\end{bmatrix} = 
\begin{bmatrix}
1 & 0  \\
0 & 1 
\end{bmatrix} 
\end{equation}
 
\begin{definition}
Let $H[0:m-1,0:n-1]$  be a matrix.
$H^{\nearrow}[0:m,0:n]$ constructed as sum of element that lies below and left given cell $i,j$ in matrix $H$ called \emph{dominance-sum} matrix of $H$
\end{definition}

The example dominance sum matrix:
\begin{equation}
\begin{bmatrix}
1 & 0  \\
0 & 1 
\end{bmatrix}^{\nearrow} =
\begin{bmatrix}
0+0+0 & 1 & 1+1 \\
0+0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix} =
\begin{bmatrix}
0 & 1 & 2 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix}
\end{equation}

In~\cite{tiskin} is proved that $H_{a,b}$ is unit anti Monge.
Also, it is proved that this matrix may be decomposed to permutation matrix, i.e into a \emph{cross-difference} matrix.
It allows to store $H_{a,b}$ implicitly and query any element of $H_{a,b}$ via dominance sum query (orthogonal range queries).
Thus, there are several ways to store matrix $H_{a,b}$ or one of its quadrants implicitly.
A simple storing of two lists of permutations requires $O(|a|+|b|)$ space and time providing $O(|a|+|b|)$ orthogonal range query time complexity (since it is necessary to check how many points dominated by given point), whereas more sophisticated approach requires $O(|a|+|b|)$ space with $O( (|a|+|b|) \sqrt{\log{ (|a| +|b|)} } )$ preprocessing time and allows to query any point of $H$ in $O(\frac{\log (|a|+|b|)}{\log \log (|a|+|b|)})$ time.  
Moreover, while arbitrary query has a non-constant time complexity, the following proposition stands that adjacent matrix elements can be queried within a constant time~\cite{.}.
\begin{proposition}
  Given a permutation matrix $P$ and a value $P^{\nearrow}[i; j]$,
  values $P^{\nearrow}[i +- 1; j], P^{\nearrow}[i; j +- 1]$, where they exist,
  can be queried in time $O(1)$.
\end{proposition}

We particularly interested in the left lower quadrant that refers to string-substring problem:
\begin{equation}
H_{a,b}^{str-sub}[i,j] = lcs(a,b[i,j]) \quad i,j \in [0,|b|] 
\end{equation}

\subsection{Semi-local sa}
The semi-local sequence alignment is a generalization of semi-local lcs in the same manner as sequence alignment is a generalization of lcs.
Given two strings $a$ and $b$ and scoring scheme $w=(w_{+},w_{0},w_{-})$\footnote{Normally assumed that $w_{+}\geq 0$, $w_{0} < w_{+}$ and $w_{-}\leq \frac{w_{0}}{2}$}  semi-local sa asks about sa scores for the following:
\begin{itemize}
\item \emph{string-substring}: whole $a$ against every substring of $b$
\item \emph{substring-string}: whole $b$ against every substring of $a$
\item \emph{prefix-suffix}: every prefix of $a$ against every suffix of $b$
\item \emph{suffix-prefix}: every prefix of $b$ against every suffix of $a$
\end{itemize} 
The \emph{associated matrix} for semi-local sa is defined by analogy as for semi-local lcs.

Semi-local sa problem can be solved by reducing to semi-local lcs.
First, note that the scoring scheme from~\ref{formula:sa} may be simplified by so-called normalization~\cite{.}:\footnote{ Assumed that $w_{+}-2w_{-}>0$}\begin{equation}\label{weightNormalization}
  \begin{array}{ll}
    w &= (w_{+}, w_{0} , w_{-}) = (w_{+} +2x , w_{0} + 2x , w_{-} + x), \forall x\\
    &= ( \frac{w_{+} +2x}{w_{+} +2x} , \frac {w_{0} + 2x}{w_{+} +2x} , \frac{w_{-} + x}{w_{+} +2x})
    \overset{x=-w_{-}}{\to} (1,\frac{\mu}{v} ,0) 
  \end{array}
\end{equation}
The resulted scoring scheme $w_{normalized} = (1,\frac{\mu}{v} ,0)$ is called the \emph{normalized scoring scheme}.
Then to query initial score $sa$ for scoring scheme $w$ knowing $sa_{normalized}$ for $w_{normalized}$ one need to apply reverse regularization:
\begin{equation}
  \begin{aligned}
    sa(a,b,w) = sa_{normalized}  (w_{+} - 2w_{-}) +  w_{-} (|a| + |b|)
  \end{aligned}
\end{equation}

Finally, the blown-up technique\footnote{Each of the input length increases by factor $v$.} is applied after reducing the scoring scheme.
At this stage, we have transitioned to semi-local lcs problem which can be solved in $O(|a| \times |b| \times v^2)$ time by algorithm from~\cite{.}.
Finally, unit-Monge matrix multiplication algorithm reduces time complexity in $v$ times\todo{~\cite{.}}.

%\todo{Nonetheless, only one of the described algorithm time %complexity increases in $v^{2}$ times, the second one only $v$. %\red{Bad sentecnce}.
%The space complexity also increses by factor $v$. }
%(we store permitation matrix of size $(v(|a|+|b|) \times v(|a|%%+|b|) $) 
%Thus, the rinning time and space complexity of semi-local sa is %$(v(|a|+|b|) \times v(|a|+|b|) $ and 
%\todo{The detalied description can be found in TISKIN BOOK~%\cite{.}.}


\subsection{Range maximum/minimum queries}

Range maximum (minimum) que\-ries (\emph{rmq}, submatrix query) refer to a search of the maximum (minimum) element in submatrix $[i_{1}:i{_2}]\times [j_{1}:j{_2}]$ of a given matrix $M$ of size $n \times n$.
The associated data structure that can report maximum (minimum) element in any submatrix query is called \emph{range maximum (minimum) data structure}.
In general, the optimal space complexity of this data structure is $O(n^2)$ since storing the matrix requires at least $O(n^2)$ space.
Nonetheless, it is not the case if considering Monge matrices which can be stored implicitly.
% Nonetheless, it is not the case when considering Monge matrices.the situation is changed if we  consider special cases such as Monge matrices.
%Monge matrices can  be (and often) stored implicilty.
\emph{rmq} on Monge matrices has been a source of research over the past decades~\cite{}.
The recent research provides the following results~\cite{.}.

\begin{theorem}~\cite{.}
Given an $n \times n$ Monge matrix $M$, a data structure of size $O(n)$ can be constructed in $O(n \times \log n)$ time to answer submatrix maximum queries in $O(\log \log n)$ time when random access to Monge matrix is $O(1)$.
\end{theorem}

%\begin{theorem}~\cite{.}
%Given an $n \times n$ staircase\footnote{\red{Staircase matrix %is the matrix where }} Monge matrix $M$, a data structure of %size $O(n)$ can be constructed in $O(n \log n$) time to answer %submatrix maximum queries in $O(\log \log n$) time when random %access to Monge matrix is $O(1)$.
%end{theorem}

\begin{theorem}~\cite{.}
  Given an $n \times n$ partial\footnote{Partial matrix is the matrix where some entries undefined, but defined entries in each coloumn and row are contigous} Monge matrix\footnote{A partial totally Monge matrix is a partial matrix whose defined entries satisfy the Monge property.} $M$, a data structure of size $O(n)$ can be constructed in $O(n \times \log n$) time to answer submatrix maximum queries in $O(\log \log n$) time when random access to Monge matrix is $O(1)$.
\end{theorem}

If the random access to Monge matrix is non-constant, say $O(\beta)$, then the results above are satisfied with time construction and query times increased by factor $\beta$.
  
%\begin{definition}
%Given  query of type $i_{1},i_{2},j_1,j_2, i_1,i_2 \in [0,]$
%\end{definition}
%\emph{rmq}  

\subsection{Near-duplicate detection algorithm}
First, we denote several parameters are used in algorithm~\cite{.}.
Let $k$, a set similarity measure, be a constant in interval $[\frac{1}{\sqrt{3}},1]$.
A window $w$ of size $L_{w} = |p|/k$ is to process text $t$ with sliding window of one symbol step.
Let $k_{di} = |p|*(\frac{1}{k}+1)(1-k^2)$ be a threshold value for edit distance,
$I$~---~interval of size $[|p|k,\frac{|p|}{k}]$ that sets boundaries for lenght of matching substrings,
$d_{di}$~---~function measures similarity between given strings.

The algorithm (see Algorithm~\ref{alg:luciv}) comprises of three phases.
At the first phrase text $t$ is processed with sliding window of size $L_{w}$ with one symbol step.
Further, substrings that corrsepond to window $w$ are compared using edit distance\footnote{
  Authors of~\cite{.} used lcs edit distance~---~where  operations substituion,removoval, addition of one symbol costs 2,1,1 respectively}
and if $d_{di}(p,t_{w}) \leq k_{di}$, i.e. close enough, then they are added to set $W_{1}$ to be further proceeded. 
On the second phase each of the detected substrings in $W_{1}$ shrunk, i.e their length can be decreased.
More precisely, each element $w \in W_{1}$ is iterated over to find its longest substring such that its length falls in interval $I$ and it is the most similar to pattern $p$.
Set $W_2$ stands for the result of this phase.
At the final third phase, set $W_{2}$ is filtered by removing elements that fully contains or duplicates any other element of set $W_{2}$.

\begin{algorithm}[!t]
\caption{PATTERN BASED NEAR DUPLICATE
SEARCH ALGORITHM}\cite{}
\label{alg:luciv}
Input: pattern $p$, text $t$, $k$ ---similiarity measure\\
Output: Set of non-intersected clones of pattern $p$ in text $t$\\
Pseudocode:
\begin{algorithmic}[1]
\STATE{$W_1 = \emptyset$}
\COMMENT{1st phase}
\FOR{$\forall w_{1}:w_{1} \in t \land |w_{1}|=L_{w} $}
\IF{$d_{di} \leq k_{di}$}
\STATE{add $w_1$ to $W_{1}$}
\ENDIF
\ENDFOR
\STATE{$W_{2} = \emptyset$}
\COMMENT{2d phase}

\FOR{$w \in  W_{1}$}
\STATE{$w^{'}_{2} = w$}
\FOR{$l \in I $}
\FOR{$\forall w_2: w_2 \subseteq  w \land |w_2| = l$} 
\IF{$Compare(w_{2}, w_{2}^{'},p)$}
\STATE{$w_{2}^{'} = w_2$} 
\ENDIF
\ENDFOR
\ENDFOR
\STATE{add $w_{2}^{'}$ to $W_{2}$}
\ENDFOR

\STATE{ $W_3 = UNIQUE(W_2)$}

\COMMENT{3rd phase }
\FOR{$w \in W_3$}
\IF{$\exists w^{'} \in W_3:w \subset w^{'} $}
\STATE{ $remove$ $w$ $from$ $W_3$}
\ENDIF
\ENDFOR
\RETURN $W_3$

\end{algorithmic}
\end{algorithm}

\paragraph{Running time analysis}
At first phase at most $O(|t|-\frac{|p|}{k})=O(|t|)$ windows of size $L_{w}$ will be processed.
Since edit distance running time complexity is $O(|p|^2)$, the overall time complexity of the first phase is $O(|t| \times |p|^2)$.
Note, cardinality of set $W_{1}$ could be meaused as $O(|t|)$ at worst case.
Thus, the the first loop of the second phase will be iterated over $O(|t|)$ times (line 8).
The second loop (line 10) refers to iteration over set $I$ with cardinality $O(\frac{|p|}{k}-|p|k) = O(|p|)$.
The third loop (line 11) requires at most $O(|p|)$ iterations since again a sliding window is used.
The running time complexity of \emph{Compare} operation (line 12) is at most $O(|p|^2)$ since both strings are of size $O(|p|)$.
Thus, the total running time complexity of the second phase can be estimated as $O(|p|^4|t|)$ at worst case.
Note, cardinality of set $W_{2}$ is equal to $W_{1}$ and thus at worst case is $O(|t|)$.
Finally, the third phase requires $O(|t| \times \log |t|)$ time.
Thus, the total time complexity of the algirthm can be estimated as
$O(max(|t| \times |p|^4, |t| \times \log|t|))$.

The algorithm also possesses completeness property.
To detailed description refer to~\cite{.}.
We just state the following theorem:
\begin{theorem}~\cite{Luciv}
For any $p \in t$ , $k \in (\frac{1}{ \sqrt{3}} , 1]$, and near duplicate group $G$ of fragment $p$ with similarity $k$ the criterion of \emph{completeness} is satisfied with respect to the output of phase 2.
\end{theorem}
