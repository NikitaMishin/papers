\section{Preliminaries}
\label{section:preliminaries}

The section provides some backgraound, definitions, and theoremes required for
developed algortithms.

\subsection{Approximate pattern matching}
The approximate pattern matching problem ($AMatch$) defined as follows.
Given text $t$, pattern $p$, simularity function $g$, and some threshold $h$ the \emph{approximate pattern matching} problem asks for all substrings from text $t$ that have similarity score  with given pattern $p$ at least $h$ according to a function $g$.  

There exist different extensions and particular cases of the problem.
The most familiar case, \emph{complete approximate pattern matching} (\emph{CompleteAMatch}) that asks for all substrings of text $t$ that are exact clones of pattern $p$.
CompleteAMatch can be solved by a number well-knonwn algorithms such as Aho--Korasic, Bouer--Murr, Knuth--Morris--Pratt, and so on.
The optimal CompleteAMatch solution running time is $O(|p|+|t|)$~\cite{.}.
Other special cases of AMatch are \emph{approximate pattern matching with k mismatches}~\cite{.}, search of \emph{pattern with wildcard symbols}~\cite{.}, multidimensional \emph{AMatch}~\cite{.}, AMatch with a length constraint on the resulting substrings~\cite{.}, and many more~\cite{.}.

One of the common approaches to solving AMAtch problem is the utilization of string similarity problem solutions.
Latter represents a set of fundamental problems such as \emph{edit distance}, \emph{longest common subsequence}, and \emph{sequence alignment}.
\todo{In the paper, we primarily focus on the usage of the latter two when developing algorithms.}

Recently there has been developed an algorithm for solving one particular \emph{AMatch} extension~---~the one with a length constarint on the resulting substrings~\cite{.}.
\todo{Describe why the problem is actual}.
Despite the algorithm \todo{has poor result in terms of running time complexity}, the proposed solution possesses a completeness property, i.e it founds \emph{all} non-intersected clones of a given pattern with specified similarity threshold and length constraint on matching substrings.
Precisely due to the completeness property the algorithm is of intereset in this paper.
The algorithm is described in section~\ref{.} while the developed improved version is presented in section~\ref{.}.

\subsection{Global lcs and sa}
% Semi-local lcs and sa are extensions of \emph{lcs} and \emph{sa} problems, respectively.

\begin{definition}
Given two strings $a$ and $b$  the longest common subsequence (\emph{LCS}) problem ask for the maximal length of the longest common subsequence of $a$ and $b$ ($lcs(a,b)$).
\end{definition} 
\todo{In other words, \emph{LCS} problem asks about maximal $lcs$ score of two given string $a$ and $b$ ($lcs(a,b)$)}.

\begin{definition}
Given two strings $a$ and $b$ and scoring scheme $w=(w_{+},w_{0},w_{-})$ the sequence alignment (\emph{SA}) problem ask for the maximal alignment score between $a$ and $b$ ($sa(a,b,w)$).
\end{definition}

Scoring scheme determines how to calculate the alignment score of two aligned sequences.
If a pair of character in aligned sequences are matched (equal) then this pair contributes to the final alignment score $w_{+}$, if their mismatch it contributes $w_{0}$.
If symbol $\alpha$ of one of the sequences is not aligned with any other symbol from another sequence it means that $\alpha$ is aligned with a $gap$.
In this case, this pair contributes $w_{0}$.
The final scoring scheme calculates as follows:
\begin{equation}\label{formula:sa}
\begin{array}{ll}
    sa(a,b,w) &= w_{+}k^{+} + w_{0}k^{0} + w_{-} (|a| + |b| - 2k^{+} - 2k^{0}) \\
    &= k^{+} (w_{+} - 2w_{-} ) + k^{0}  (w_{0} - 2w_{-}) + w_{-}(|a| + |b|)
\end{array}
\end{equation}
where $k^{+}$ stands for the number of matched symbols, $k^{-}$ --- the number of mismatched symbols.
Note that \emph{LCS} is a special case of \emph{SA} with scoring scheme be $(1,0,0)$.
Both described problems can be solved with usual dynamic programming algorithm having running time complexity $O(|a|\times|b|)$.

\subsection{Semi-local lcs}

$LCS$ and $SA$ allow one to find how much whole given strings are similar, i.e. how similar two string in a global sense.
Sometimes this is not enough.
There exist \emph{fully local} and \emph{semi-local} generalizations of these problems.
In the paper, we focus on \emph{semi-local} ones due to natural applicability to approximate pattern matching.
The key feature of semi-local problems is that they can be solved within the same time as global ones while collects more information about strings similarity.
More precisely, given two strings $a$ and $b$ the semi-local lcs asks about $lcs$ scores for following:
\begin{itemize}
\item \emph{string-substring}: whole $a$ against every substring of $b$,
\item \emph{substring-string}: whole $b$ against every substring of $a$,
\item \emph{prefix-suffix}: every prefix of $a$ against every suffix of $b$,
\item \emph{suffix-prefix}: every prefix of $b$ against every suffix of $a$.
\end{itemize} 
Formally, semi-local lcs can be defined via \emph{semi-local lcs matrix} as follows.
\begin{definition}
The \emph{semi-local lcs matrix}  $H_{a,b}$ for strings $a$, $b$ defined as follows:
\begin{equation}
  H_{a,b}[i,j] = \textbf{ if } j\leq i \textbf{ then } j-i \textbf{ else } lcs(a,b^{pad}[i,j]) 
\end{equation} 
where $i \in [-|a|:|b|], j \in [0:|a|+|b|] $, $b^{pad}= ?^{|a|}\ b\ ?^{|a|}$, $?$--- wildcard symbol that matches any other symbol.
\end{definition}
The semi-local lcs matrix $H_{a,b}$ comprises from four quadrants associated with described subproblems:
\begin{equation}
  H_{a,b} = \begin{bmatrix}
    H_{a,b}^{suf-pre} & H_{a,b}^{sub-str} \\
    H_{a,b}^{str-sub} & H_{a,b}^{pre-suf} 
  \end{bmatrix}    
\end{equation}

\subsection{Matrix properties}

\begin{definition}
Matrix $H$ called (anti) Monge matrix if

\begin{displaymath}
  \forall i,i',j,j':\ i\leq i^{'}, j \leq j^{'}\ .\ H[i,j]+H[i^{'},j^{'}] (\geq)\leq H[i,j^{'}]+H[i^{'},j]
\end{displaymath}
\end{definition}

\begin{definition}
Let $H[0:m,0:n]$  be a matrix.
$H^{\square}[0:m-1,0:n-1]$ constructed as a result of taken cross difference between secondary and first diagonal for all adjacent 2 by 2 squares called \emph{cross-difference} matrix of  $H$
\end{definition}


\begin{definition}
Matrix $H$ called unit anti Monge matrix if $H$ is (anti) Monge matrix and its \emph{cross-difference matrix} $(-)H^{\square}$ is permutation matrix.
\end{definition}
The example of unit anti Monge matrix is following:
\begin{equation}
\begin{bmatrix}
0 & 2 & 3 \\
0 & 1 & 2 \\
0 & 1 & 1
\end{bmatrix} ^ { \square} =
\begin{bmatrix}
(2 + 0) - (1 + 0)  & (3 + 1) - (2 + 2)  \\
(1 + 0) - (1 + 0) &  (2 + 1) - (1 + 1) 
\end{bmatrix} = 
\begin{bmatrix}
1 & 0  \\
0 & 1 
\end{bmatrix} 
\end{equation}
 
\begin{definition}
Let $H[0:m-1,0:n-1]$  be a matrix.
$H^{\nearrow}[0:m,0:n]$ constructed as sum of element that lies below and left given cell $i,j$ in matrix $H$ called \emph{dominance-sum} matrix of $H$
\end{definition}

The example dominance sum matrix:
\begin{equation}
\begin{bmatrix}
1 & 0  \\
0 & 1 
\end{bmatrix}^{\nearrow} =
\begin{bmatrix}
0+0+0 & 1 & 1+1 \\
0+0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix} =
\begin{bmatrix}
0 & 1 & 2 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix}
\end{equation}

In~\cite{tiskin} is proved that $H_{a,b}$ is unit anti Monge.
Also, it is proved that this matrix may be decomposed to permutation matrix, i.e into a \emph{cross-difference} matrix.
It allows to store $H_{a,b}$ implicitly and query any element of $H_{a,b}$ via dominance sum query (orthogonal range queries).
Thus, there are several ways to store matrix $H_{a,b}$ or one of its quadrants implicitly.
A simple storing of two lists of permutations requires $O(|a|+|b|)$ space and time \todo{with $O(|a|+|b|)$} orthogonal range queries (need to check how many points dominated by given point), whereas more sophisticated approach requires $O(|a|+|b|)$  space with $O( (|a|+|b|) \sqrt{\log{ (|a| +|b|)} } )$ preprocessing time and allows to query any point of $H$ in $O(\frac{\log (|a|+|b|)}{\log \log (|a|+|b|)})$ time.  

\todo{The one useful proposition of $H$ is the following}~\cite{.}.
\begin{proposition}
  Given a permutation matrix $P$ and a value $P^{\nearrow}[i; j]$,
  values $P^{\nearrow}[i +- 1; j], P^{\nearrow}[i; j +- 1]$, where they exist,
  can be queried in time $O(1)$.
\end{proposition}

We particularly interested in the left lower quadrant that refers to string-substring problem:
\begin{equation}
H_{a,b}^{str-sub}[i,j] = lcs(a,b[i,j]) \quad i,j \in [0,|b|] 
\end{equation}

\todo{There exists several algorithms \red{second on,recursive not described as i see} that solve \emph{semi-local lcs}.
Both have the optimal running time $O(|a| \times |b|)$ for given dynamic problem\cite{}\red{Impossibility faster then pt}.}

\subsection{Semi-local sa}
The semi-local sequence alignment (sa) is a generalazation of semi-local lcs in same sense as sequence alignemnt is generelaztion of lcs.

Given two strings $a$ and $b$ and scoring scheme $w=(w_{+},w_{0},w_{-})$  the semi-local sa  asks about
sa scores for following:
\begin{enumerate}
\item \emph{string-substring}: whole $a$ against every substring of $b$
\item \emph{substring-string}: whole $b$ against every substring of $a$
\item \emph{prefix-suffix}: every prefix of $a$ against every suffix of $b$
\item \emph{suffix-prefix}: every prefix of $b$ against every suffix of $a$
\end{enumerate} 

The associated matrix for \emph{semi-local sa} is defined analogously as for \emph{semi-local lcs}.

The approach for solving \emph{semi-local sa} is as follows.
The problem reduced to \emph{semi-local lcs}.
First, note that scoring scheme in \ref{formula:sa}
may be simplified by so called normalization\cite{}:
\begin{equation}\label{weightNormalization}
    \begin{aligned}
    w = (w_{+}, w_{0} , w_{-}) \xrightarrow{} (w_{+} +2x , w_{0} + 2x , w_{-} + x) =\\ ( \frac{w_{+} +2x}{w_{+} +2x} , \frac {w_{0} + 2x}{w_{+} +2x} , \frac{w_{-} + x}{w_{+} +2x})_{x=-w_{-}} = (1,\frac{\mu}{v} ,0) 
    \end{aligned}
\end{equation}
The resulted scoring scheme $w_{normalized} = (1,\frac{\mu}{v} ,0)$ called normalized scoring scheme.

Then to query initial score $sa$ for scoring scheme $w$
knowing $sa_{normalized}$ for $w_{normalized}$ you need to apply reverse regularization:
\begin{equation}
    \begin{aligned}
    sa(a,b,w) = sa_{normalized}  (w_{+} - 2w_{-}) +  w_{-} (|a| + |b|)
\end{aligned}
\end{equation}

The blown-up tecnhique is applied after reducing scoring scheme
which increases both input strings in $v$ times.
Nonetheless, only one of the described algorithm time complexity increases in $v^{2}$ times, the second one only $v$. \red{Bad sentecnce}.
The space complexity also increses by factor $v$. 
%(we store permitation matrix of size $(v(|a|+|b|) \times v(|a|%%+|b|) $) 

%Thus, the rinning time and space complexity of semi-local sa is %$(v(|a|+|b|) \times v(|a|+|b|) $ and 

For detalied description we refer readers to TISKIN BOOK\cite{}.

 
\subsection{Range maximum/minimum queries}

Range maximum/minimum queries (\emph{rmq}) (submatrix query)  refers to search maximum/minimum element in submatrix $[i_{1}:i{_2}]\times [j_{1}:j{_2}]$ of given matrix $M$ of size $n\times n$.
The associated data strucutre that can report maximum/minimum element in any  submatrix query called \emph{range maximum/minimum data structure}.

For the generic case of Matrix $M$ it is not possible to
achieve running time faster then $O(n^2)$ due to fact that storing matrix $M$ requires $O(n^2)$.

Nonetheless, the situation is changed if we  consider special cases such as Monge matrices.
%Monge matrices can  be (and often) stored implicilty.
There have been several researches over several decades about 
rmq on monge matrices \cite{}.

The recent research achives following result\cite{}.

\begin{theorem}\cite{}
Given an $n \times n$ Monge matrix $M$, a data structure of size $O(n)$ can be constructed
in $O(n \log n)$ time to answer submatrix maximum queries in $O(\log \log n)$ time when random access to Monge matrix is $O(1)$.
\end{theorem}


\begin{theorem}\cite{}
Given an $n \times n$ staircase\footnote{Defintion} Monge matrix $M$, a data structure of size $O(n)$ can be
constructed in $O(n \log n$) time to answer submatrix maximum queries in $O(\log \log n$) time when random access to Monge matrix is $O(1)$.
\end{theorem}


\begin{theorem}\cite{}
Given an $n \times n$ partial Monge matrix\footnote{Definition} $M$, a data structure of size $O(n)$ can be
constructed in $O(n \log n$) time to answer submatrix maximum queries in $O(\log \log n$) time when random access to Monge matrix is $O(1)$.
\end{theorem}

The above results applies both to range  minimum queries and to monge matrices with non-constant access $O(\beta)$ to queries.
The latter one, costs in  increased construction time and query time by factor $\beta$. 

  
%\begin{definition}
%Given  query of type $i_{1},i_{2},j_1,j_2, i_1,i_2 \in [0,]$
%\end{definition}
%\emph{rmq}  





\subsection{Near-duplicate detection algorithm}
First, we denote several parameters, that is used in algorithm \cite{}.
$k$--- constant in interval $[\frac{1}{\sqrt{3}},1]$ that set similarity measure.  
A window $w$ of size $L_{w} = |p|/k$ is to process text $t$ with
sliding window of one symbol step.
$k_{di} = |p|*(\frac{1}{k}+1)(1-k^2)$ is threshold value for edit distance.
$I$ --- interval of size $[|p|k,\frac{|p|}{k}]$ that set boundaries for lenght of matching substrings.
$d_{di}$ --- function that measure similarity between two strings.

The algorithm comprises of three phases.

At the first phrase  text $t$ is processed with sliding window of size $L_{w}$ with one symbol step.
Further, substrings that corrsepond to window $w$ compared using edit distance\footnote{Authors of \cite{} used lcs edit distance --- where  operations substituion,removoval, addition of one symbol costs 2,1,1 respectively} and if $d_{di}(p,t_{w}) \leq k_{di}$ i.e  close enough, then they saved to set $W_{1}$ to be further proceeded. 

On the second phase  each of the detected substrings in $W_{1}$ are shrunk i.e they lenght could be decreased.
More preciesely, within each of the element of $W_{1}$ the largest one substring with legnth fall in $I$ that most similar to pattern $p$ according to $d_{di}$ is selected.
The set $W_{2}$ is a result of this phase.

At the final third phase  set $W_{2}$ iterated over to remove elements that fully contains in ohter elements of $W_{2}$ or duplicates.


\begin{algorithm}[H]
\caption{PATTERN BASED NEAR DUPLICATE
SEARCH ALGORITHM}\cite{}
\label{alg:luciv}
Input: pattern $p$, text $t$, $k$ ---similiarity measure\\
Output: Set of non-intersected clones of pattern $p$ in text $t$\\
Pseudocode:
\begin{algorithmic}[1]
\STATE{$W_1 = \emptyset$}
\COMMENT{1st phase}
\FOR{$\forall w_{1}:w_{1} \in t \land |w_{1}|=L_{w} $}
\IF{$d_{di} \leq k_{di}$}
\STATE{add $w_1$ to $W_{1}$}
\ENDIF
\ENDFOR
\STATE{$W_{2} = \emptyset$}
\COMMENT{2d phase}

\FOR{$w \in  W_{1}$}
\STATE{$w^{'}_{2} = w$}
\FOR{$l \in I $}
\FOR{$\forall w_2: w_2 \subseteq  w \land |w_2| = l$} 
\IF{$Compare(w_{2}, w_{2}^{'},p)$}
\STATE{$w_{2}^{'} = w_2$} 
\ENDIF
\ENDFOR
\ENDFOR
\STATE{add $w_{2}^{'}$ to $W_{2}$}
\ENDFOR

\STATE{ $W_3 = UNIQUE(W_2)$}

\COMMENT{3rd phase }
\FOR{$w \in W_3$}
\IF{$\exists w^{'} \in W_3:w \subset w^{'} $}
\STATE{ $remove$ $w$ $from$ $W_3$}
\ENDIF
\ENDFOR
\RETURN $W_3$

\end{algorithmic}
\end{algorithm}

\paragraph{Running time analysis}
\emph{1st phase}.The first phase requires at most  $O(|t||p|^2)$ due to fact
that computing cost of edit distance is $|p|^2$ for strings of size $O(|p|)$ and we need to process $O(|t|-\frac{|p|}{k})=O(|t|)$ windows.
\emph{2nd phase}. 
The cardinality of set $W_{1}$  at worst case be $O(|t|)$.
Thus, first loop will be iterated over $O(|t|)$ times.
The second loop refers to iteration over $I$ set with cardinality $O(\frac{|p|}{k}-|p|k) = O(|p|)$.
The third loop requires at most $O(|p|)$ since we again goes with sliding window.
The compare operation requires at most $O(|p|^2)$ since both
stirng of size $O(|p|)$.
Thus, the total running time compleixty of second phase is $O(|p|^4|t|)$ at worst case.

\emph{3rd phase}. 
Note that cardinality of set $W_{2}==W_{1}$ and thus at worst case is $O(|t|)$.
Then, this phase requires $O(|t| \log |t|)$ time. 

Thus,the total time complexity of algirthms estimates as 
$O(max(|t||p|^4,|t| \log|t|))$.

Algorithm also possesses completnesses property defined in their paper. 
To detailed description refer to \cite{}.
We just state their following theorem:
\begin{theorem}\cite{Luciv}
For any $p \in t$ , $k \in (\frac{1}{ \sqrt{3}} , 1]$ and near
duplicate group $G$ of fragment $p$ with similarity $k$ the criterion of \emph{completeness} is satisfied in
respect to the output of phase 2.
\end{theorem}

Theorem states that if pattern $p$ is presented in text $t$ then all duplicates of it would be found in text $t$. 
